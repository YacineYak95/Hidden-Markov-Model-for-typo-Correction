{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de Markov Caché du second ordre\n",
    "\n",
    "### Application à la correction de typos dans des textes\n",
    "\n",
    "#### Les typos sont maintenant des insertions de caractères au lieu de substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation de la classe HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<unk>\"  # token to map all out-of-vocabulary words (OOVs)\n",
    "INS = \"<ins>\"\n",
    "UNKid = 0  # index for UNK\n",
    "#INSid = 1\n",
    "epsilon = 1e-100\n",
    "\n",
    "\n",
    "class HMM:\n",
    "    def __init__(self,\n",
    "                 state_list,\n",
    "                 observation_list,\n",
    "                 head_transition_proba=None,\n",
    "                 transition_proba=None,\n",
    "                 observation_proba=None,\n",
    "                 initial_state_proba=None,\n",
    "                 transition_head_proba=None,\n",
    "                 smoothing_obs=0.01):\n",
    "        \"\"\"\n",
    "            Builds a Hidden Markov Model\n",
    "            * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            * transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            * observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            * initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "\n",
    "        print (\"HMM creating with: \")\n",
    "\n",
    "        self.N = len(state_list)  # number of states\n",
    "        self.M = len(observation_list)  # number of possible emissions\n",
    "\n",
    "        print (str(self.N) + \" states\")\n",
    "        print (str(self.M) + \" observations\")\n",
    "\n",
    "        self.omega_Y = state_list\n",
    "        self.omega_X = observation_list\n",
    "\n",
    "        if head_transition_proba is None:\n",
    "            self.head_transition_proba = zeros((self.N, self.N))\n",
    "        else:\n",
    "            self.head_transition_proba = head_transition_proba\n",
    "\n",
    "        if transition_proba is None:\n",
    "            self.transition_proba = zeros((self.N, self.N**2))\n",
    "        else:\n",
    "            self.transition_proba = transition_proba\n",
    "\n",
    "        if observation_proba is None:\n",
    "            self.observation_proba = zeros((self.M, self.N))\n",
    "        else:\n",
    "            self.observation_proba = observation_proba\n",
    "\n",
    "        if initial_state_proba is None:\n",
    "            self.initial_state_proba = zeros((self.N, ))\n",
    "        else:\n",
    "            self.initial_state_proba = initial_state_proba\n",
    "\n",
    "        self.make_indexes(\n",
    "        )  # build indexes, i.e the mapping between token and int\n",
    "        self.smoothing_obs = smoothing_obs\n",
    "\n",
    "    def make_indexes(self):\n",
    "        \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "        self.Y_index = {}\n",
    "        for i in range(self.N):\n",
    "            self.Y_index[self.omega_Y[i]] = i\n",
    "        self.X_index = {}\n",
    "        for i in range(self.M):\n",
    "            self.X_index[self.omega_X[i]] = i\n",
    "\n",
    "    def get_observationIndices(self, observations):\n",
    "        \"\"\"return observation indices, i.e \n",
    "            return [self.O_index[o] for o in observations]\n",
    "            and deals with OOVs\n",
    "            \"\"\"\n",
    "        indices = zeros(len(observations), int)\n",
    "        k = 0\n",
    "        for o in observations:\n",
    "            if o in self.X_index:\n",
    "                indices[k] = self.X_index[o]\n",
    "            else:\n",
    "                indices[k] = UNKid\n",
    "            k += 1\n",
    "        return indices\n",
    "\n",
    "    def data2indices(self, sent):\n",
    "        \"\"\"From one tagged sentence of the brown corpus: \n",
    "            - extract the words and tags \n",
    "            - returns two list of indices, one for each\n",
    "            -> (wordids, tagids)\n",
    "            \"\"\"\n",
    "        wordids = list()\n",
    "        tagids = list()\n",
    "        for couple in sent:\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "            if wrd in self.X_index:\n",
    "                wordids.append(self.X_index[wrd])\n",
    "            else:\n",
    "                wordids.append(UNKid)\n",
    "            tagids.append(self.Y_index[tag])\n",
    "        return wordids, tagids\n",
    "\n",
    "    def observation_estimation(self, pair_counts):\n",
    "        \"\"\" Build the observation distribution: \n",
    "                observation_proba is the observation probablility matrix\n",
    "                    [b_ki],  b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "                \n",
    "                pair_counts is dictionary with \n",
    "                - key : a tuple (word,tag)\n",
    "                - value: the associated count\n",
    "                \n",
    "                We just need to fill the matrix and normalize the count in the right way: \n",
    "                - one column (i constant) is the distrib. of word given a tag\n",
    "                - just normalize the column, i.e sum over the row (axis=0)\n",
    "            \"\"\"\n",
    "        # fill with counts\n",
    "        for pair in pair_counts:\n",
    "            wrd = pair[0]  # get word\n",
    "            tag = pair[1]  # get tag\n",
    "            cpt = pair_counts[pair]  # get the count\n",
    "            # get word index (row), deal with OOV\n",
    "            k = 0  # for <unk>\n",
    "            if wrd in self.X_index:\n",
    "                k = self.X_index[wrd]\n",
    "            # get tag  index (column)\n",
    "            i = self.Y_index[tag]\n",
    "            # fill the matrix\n",
    "            self.observation_proba[k, i] = cpt\n",
    "        # normalize\n",
    "        self.observation_proba = self.observation_proba + self.smoothing_obs\n",
    "        self.observation_proba = self.observation_proba / self.observation_proba.sum(\n",
    "            axis=0).reshape(1, self.N)\n",
    "\n",
    "    def head_transition_estimation(self, head_trans_counts):\n",
    "        \"\"\" Buid the transition distribution \"\"\"\n",
    "        # fill with counts\n",
    "        for pair in head_trans_counts:\n",
    "            i = self.Y_index[pair[0]]\n",
    "            j = self.Y_index[pair[1]]\n",
    "            self.head_transition_proba[j, i] = head_trans_counts[pair]\n",
    "        # normalize\n",
    "        self.head_transition_proba = self.head_transition_proba + self.smoothing_obs\n",
    "        self.head_transition_proba = self.head_transition_proba / self.head_transition_proba.sum(\n",
    "            axis=0).reshape(1, self.N)\n",
    "\n",
    "    def transition_estimation(self, trans_counts):\n",
    "        \"\"\" Build the transition distribution \"\"\"\n",
    "        # fill with counts\n",
    "        for pair in trans_counts:\n",
    "            i = self.Y_index[pair[0][0]]\n",
    "            j = self.Y_index[pair[0][1]]\n",
    "            k = self.Y_index[pair[1]]\n",
    "            self.transition_proba[k, (i * self.N + j)] = trans_counts[pair]\n",
    "        # normalize\n",
    "        self.transition_proba = self.transition_proba + self.smoothing_obs\n",
    "        self.transition_proba = self.transition_proba / self.transition_proba.sum(\n",
    "            axis=0).reshape(1, self.N**2)\n",
    "\n",
    "    def init_estimation(self, init_counts):\n",
    "        \"\"\"Build the init. distribution\"\"\"\n",
    "        # fill with counts\n",
    "        for tag in init_counts:\n",
    "            i = self.Y_index[tag]\n",
    "            self.initial_state_proba[i] = init_counts[tag]\n",
    "        # normalize\n",
    "        self.initial_state_proba = self.initial_state_proba / sum(\n",
    "            self.initial_state_proba)\n",
    "\n",
    "    def supervised_training(self, pair_counts, head_trans_counts, trans_counts,\n",
    "                            init_counts):\n",
    "        \"\"\" Train the HMM's parameters. This function wraps everything \"\"\"\n",
    "        self.observation_estimation(pair_counts)\n",
    "        self.head_transition_estimation(head_trans_counts)\n",
    "        self.transition_estimation(trans_counts)\n",
    "        self.init_estimation(init_counts)\n",
    "\n",
    "    def viterbi(self, obsids):\n",
    "        \"\"\" Viterbi Algorithm : \n",
    "            Finding the most likely sequence of hidden states. \n",
    "            \"\"\"\n",
    "\n",
    "        T = len(obsids)\n",
    "\n",
    "        # Initialisation\n",
    "        delta = zeros(self.N, float)\n",
    "        tmp = zeros(self.N, float)\n",
    "        psi = zeros((T, self.N), int)\n",
    "\n",
    "        # Delta update\n",
    "        delta_t = zeros(self.N, float)\n",
    "\n",
    "        # Apply initial_state probabilities to the first frame\n",
    "        delta = self.observation_proba[obsids[0]] * self.initial_state_proba\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            if t == 1:\n",
    "                for i in range(self.N):\n",
    "                    for j in range(self.N):\n",
    "                        # Head\n",
    "                        tmp[j] = delta[j] * self.head_transition_proba[i, j]\n",
    "                    psi[t, i] = tmp.argmax()\n",
    "                    delta_t[i] = tmp.max() * self.observation_proba[obsids[t],\n",
    "                                                                    i]\n",
    "            else:\n",
    "                for i in range(self.N):\n",
    "                    for j in range(self.N):\n",
    "                        # Second\n",
    "                        tmp[j] = delta[j] * self.transition_proba[\n",
    "                            i, psi[t - 1, j] * self.N + j]\n",
    "                    psi[t, i] = tmp.argmax()\n",
    "                    delta_t[i] = tmp.max() * self.observation_proba[obsids[t],\n",
    "                                                                    i]\n",
    "\n",
    "            delta, delta_t = delta_t, delta\n",
    "\n",
    "        # Reconstruction\n",
    "        i_star = [delta.argmax()]\n",
    "        for psi_t in psi[-1:0:-1]:\n",
    "            i_star.append(psi_t[i_star[-1]])\n",
    "        i_star.reverse()\n",
    "\n",
    "        return i_star\n",
    "\n",
    "        \n",
    "    def print_error_rate(self, test):\n",
    "        \"\"\" Compute and print error rate on test\n",
    "            \"\"\"\n",
    "        nb_correct_before_hmm = 0\n",
    "        nb_correct_after_hmm = 0\n",
    "        total = 0\n",
    "        \n",
    "        for word in test:\n",
    "            obsids,statids = hmm.data2indices(word)\n",
    "            best_sequence = hmm.viterbi(obsids)\n",
    "\n",
    "            for (i,j) in zip(best_sequence,statids):\n",
    "                if i==j:\n",
    "                    nb_correct_after_hmm += 1\n",
    "                    \n",
    "            for(i,j) in zip(statids,obsids):\n",
    "                if i==j:\n",
    "                    nb_correct_before_hmm += 1\n",
    "\n",
    "            total += len(statids)\n",
    "\n",
    "\n",
    "        error_before_hmm = 100 - nb_correct_before_hmm * 100.0 / total\n",
    "        error_after_hmm = 100 - nb_correct_after_hmm * 100.0 / total\n",
    "\n",
    "        print(\"Error rate before HMM : {}%\".format(error_before_hmm))\n",
    "        print(\"Error rate after HMM : {}%\".format(error_after_hmm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compter les mots et les tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_counts(corpus):\n",
    "    \"\"\" \n",
    "    Build different count tables to train a HMM. Each count table is a dictionnary. \n",
    "    Returns: \n",
    "    * c_words: word counts\n",
    "    * c_tags: tag counts\n",
    "    * c_pairs: count of pairs (word,tag)\n",
    "    * c_transitions: count of tag bigram \n",
    "    * c_inits: count of tag found in the first position\n",
    "    \"\"\"\n",
    "    c_words = dict()\n",
    "    c_tags = dict()\n",
    "    c_pairs= dict()\n",
    "    c_transitions1 = dict()\n",
    "    c_transitions2 = dict()\n",
    "    c_inits = dict()\n",
    "    \n",
    "    for sent in corpus:\n",
    "        # we use i because of the transition counts\n",
    "        for i in range(len(sent)):\n",
    "            couple = sent[i]\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "            # word counts\n",
    "            if wrd in c_words:\n",
    "                c_words[wrd] = c_words[wrd] + 1\n",
    "            else:\n",
    "                c_words[wrd] = 1\n",
    "                \n",
    "            # tag counts\n",
    "            if tag in c_tags:\n",
    "                c_tags[tag] = c_tags[tag] + 1\n",
    "            else:\n",
    "                c_tags[tag] = 1\n",
    "                \n",
    "            # observation counts\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple] = c_pairs[couple] + 1\n",
    "            else:\n",
    "                c_pairs[couple] = 1\n",
    "            \n",
    "            # i >  1 -> transition counts\n",
    "            if i > 1:\n",
    "                trans = ((sent[i-2][1], sent[i-1][1]), tag) #(tag at t-2, tag at t-1, tag at t)\n",
    "                if trans in c_transitions2:\n",
    "                    c_transitions2[trans] = c_transitions2[trans] + 1\n",
    "                else:\n",
    "                    c_transitions2[trans] = 1\n",
    "                    \n",
    "            elif i == 1:\n",
    "                trans = (sent[i-1][1], tag)\n",
    "                if trans in c_transitions1:\n",
    "                    c_transitions1[trans] = c_transitions1[trans] + 1\n",
    "                else:\n",
    "                    c_transitions1[trans] = 1\n",
    "                    \n",
    "            # i == 0 -> counts for initial states\n",
    "            else:\n",
    "                if tag in c_inits:\n",
    "                    c_inits[tag] = c_inits[tag] + 1\n",
    "                else:\n",
    "                    c_inits[tag] = 1\n",
    "                    \n",
    "    return c_words, c_tags, c_pairs, c_transitions1, c_transitions2, c_inits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(c, threshold):\n",
    "    \"\"\" \n",
    "    return a vocabulary by thresholding word counts. \n",
    "    inputs: \n",
    "    * c_words : a dictionnary that maps word to its counts\n",
    "    * threshold: count must be >= to the threshold to be included\n",
    "    \n",
    "    returns: \n",
    "    * a word list\n",
    "    \"\"\"\n",
    "    voc = list()\n",
    "    voc.append(UNK)\n",
    "    voc.append(INS)\n",
    "    for w in c:\n",
    "        if c[w] >= threshold:\n",
    "            voc.append(w)\n",
    "    return voc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = ''\n",
    "\n",
    "# Données avec 10% de typos\n",
    "train10 = pickle.load(open(path+'train10.pkl', 'rb'))\n",
    "test10 = pickle.load(open(path+'test10.pkl', 'rb'))\n",
    "\n",
    "# Données avec 20% de typos\n",
    "train20 = pickle.load(open(path+'train20.pkl', 'rb'))\n",
    "test20 = pickle.load(open(path+'test20.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def insert_char(train, percentage, with_substitution=False):\n",
    "    newtext = []\n",
    "    for word in train:\n",
    "        newword=[]\n",
    "        for i in range(len(word)):\n",
    "            # Insertion\n",
    "            if (random()<percentage/100):\n",
    "                newword.append((choice(letters), INS))\n",
    "            else:\n",
    "                if(with_substitution):\n",
    "                    newword.append(word[i])\n",
    "                else:\n",
    "                    newword.append((word[i][1],word[i][1]))\n",
    "        newtext.append(newword)\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seulement des insertions\n",
    "train10 = insert_char(train10, 10)\n",
    "test10 = insert_char(test10, 10)\n",
    "train20 = insert_char(train20, 20)\n",
    "test20 = insert_char(test20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('b', 'b'), ('y', 'y')], [('t', 't'), ('h', 'h'), ('e', 'e'), ('i', 'i'), ('r', 'r')], [('o', 'o'), ('w', 'w'), ('n', 'n')], [('m', '<ins>'), ('c', 'c'), ('c', 'c'), ('o', 'o'), ('u', 'u'), ('n', 'n'), ('t', 't')], [('v', 'v'), ('i', 'i'), ('d', '<ins>'), ('l', 'l'), ('e', 'e'), ('n', 'n'), ('c', 'c'), ('e', 'e')], [('i', 'i'), ('d', '<ins>')], [('f', 'f'), ('o', 'o'), ('r', 'r')], [('a', '<ins>'), ('h', 'h'), ('e', 'e'), ('m', 'm')], [('a', 'a')], [('f', 'f'), ('o', 'o'), ('r', 'r'), ('m', 'm')]]\n"
     ]
    }
   ],
   "source": [
    "print(train10[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases totale = 30558\n",
      "Nombre de phrases de train = 29057\n",
      "Nombre de phrases de test  = 1501\n",
      "Nombre de lettres  : 26\n",
      "Nombre de tags  : 27\n",
      "Nombre de paires: 52\n",
      "Nombre de init. : 26\n",
      "{'b': 1890, 'y': 2723, 't': 12492, 'h': 6012, 'e': 16241, 'i': 9874, 'r': 7428, 'o': 10766, 'w': 2015, 'n': 8815, '<ins>': 14368, 'c': 4326, 'u': 3529, 'v': 1717, 'l': 5770, 'f': 3052, 'm': 3392, 'a': 9494, 'd': 4061, 's': 8731, 'g': 2466, 'k': 529, 'z': 109, 'j': 98, 'p': 2886, 'x': 255, 'q': 129}\n",
      "Vocabulaire :28\n"
     ]
    }
   ],
   "source": [
    "train = train10\n",
    "test = test10\n",
    "\n",
    "tot = len(train + test)\n",
    "print (\"Nombre de phrases totale = \" + str(tot))\n",
    "print (\"Nombre de phrases de train = \" + str(len(train)))\n",
    "print (\"Nombre de phrases de test  = \" + str(len(test)))\n",
    "\n",
    "cwords,ctags,cpairs,ctrans1,ctrans2,cinits = make_counts(train)\n",
    "print (\"Nombre de lettres  : \"+str(len(cwords)))\n",
    "print (\"Nombre de tags  : \"+str(len(ctags)))\n",
    "print (\"Nombre de paires: \"+str(len(cpairs)))\n",
    "print (\"Nombre de init. : \"+str(len(cinits)))\n",
    "print (ctags)\n",
    "vocab = make_vocab(cwords,10)\n",
    "print (\"Vocabulaire :\"+str(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du HMM et apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "28 states\n",
      "28 observations\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=vocab, \n",
    "          observation_list=vocab,\n",
    "          smoothing_obs = 0.001)\n",
    "\n",
    "hmm.supervised_training(cpairs,ctrans1,ctrans2,cinits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tests sur test10 avec seulement des insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate before HMM : 9.767759562841533%\n",
      "Error rate after HMM : 5.177595628415304%\n"
     ]
    }
   ],
   "source": [
    "hmm.print_error_rate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tests sur test20 avec seulement des insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "28 states\n",
      "28 observations\n",
      "Error rate before HMM : 19.89695045233958%\n",
      "Error rate after HMM : 11.389371517584323%\n"
     ]
    }
   ],
   "source": [
    "cwords,ctags,cpairs,ctrans1,ctrans2,cinits = make_counts(train20)\n",
    "\n",
    "hmm = HMM(state_list=vocab, \n",
    "          observation_list=vocab, \n",
    "          smoothing_obs = 0.001)\n",
    "\n",
    "hmm.supervised_training(cpairs,ctrans1,ctrans2,cinits)\n",
    "hmm.print_error_rate(test20)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
