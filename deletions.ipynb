{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de Markov Caché du second ordre\n",
    "\n",
    "### Application à la correction de typos dans des textes\n",
    "\n",
    "#### Les typos sont maintenant des supressions de caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "from __future__ import division\n",
    "\n",
    "UNK = \"<unk>\"  # token to map all out-of-vocabulary words (OOVs)\n",
    "INS = \"<ins>\"\n",
    "UNKid = 0  # index for UNK\n",
    "#INSid = 1\n",
    "epsilon = 1e-100\n",
    "\n",
    "\n",
    "class HMM:\n",
    "    def __init__(self,\n",
    "                 state_list,\n",
    "                 observation_list,\n",
    "                 head_transition_proba=None,\n",
    "                 transition_proba=None,\n",
    "                 observation_proba=None,\n",
    "                 initial_state_proba=None,\n",
    "                 transition_head_proba=None,\n",
    "                 smoothing_obs=0.01):\n",
    "        \"\"\"\n",
    "            Builds a Hidden Markov Model\n",
    "            * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            * transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            * observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            * initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "\n",
    "        print (\"HMM creating with: \")\n",
    "\n",
    "        self.N = len(state_list)  # number of states\n",
    "        self.M = len(observation_list)  # number of possible emissions\n",
    "\n",
    "        print (str(self.N) + \" states\")\n",
    "        print (str(self.M) + \" observations\")\n",
    "\n",
    "        self.omega_Y = state_list\n",
    "        self.omega_X = observation_list\n",
    "\n",
    "        if head_transition_proba is None:\n",
    "            self.head_transition_proba = zeros((self.N, self.N))\n",
    "        else:\n",
    "            self.head_transition_proba = head_transition_proba\n",
    "\n",
    "        if transition_proba is None:\n",
    "            self.transition_proba = zeros((self.N, self.N**2))\n",
    "        else:\n",
    "            self.transition_proba = transition_proba\n",
    "\n",
    "        if observation_proba is None:\n",
    "            self.observation_proba = zeros((self.M, self.N))\n",
    "        else:\n",
    "            self.observation_proba = observation_proba\n",
    "\n",
    "        if initial_state_proba is None:\n",
    "            self.initial_state_proba = zeros((self.N, ))\n",
    "        else:\n",
    "            self.initial_state_proba = initial_state_proba\n",
    "\n",
    "        self.make_indexes(\n",
    "        )  # build indexes, i.e the mapping between token and int\n",
    "        self.smoothing_obs = smoothing_obs\n",
    "\n",
    "    def make_indexes(self):\n",
    "        \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "        self.Y_index = {}\n",
    "        for i in range(self.N):\n",
    "            self.Y_index[self.omega_Y[i]] = i\n",
    "        self.X_index = {}\n",
    "        for i in range(self.M):\n",
    "            self.X_index[self.omega_X[i]] = i\n",
    "\n",
    "    def get_observationIndices(self, observations):\n",
    "        \"\"\"return observation indices, i.e \n",
    "            return [self.O_index[o] for o in observations]\n",
    "            and deals with OOVs\n",
    "            \"\"\"\n",
    "        indices = zeros(len(observations), int)\n",
    "        k = 0\n",
    "        for o in observations:\n",
    "            if o in self.X_index:\n",
    "                indices[k] = self.X_index[o]\n",
    "            else:\n",
    "                indices[k] = UNKid\n",
    "            k += 1\n",
    "        return indices\n",
    "\n",
    "    def data2indices(self, sent):\n",
    "        \"\"\"From one tagged sentence of the brown corpus: \n",
    "            - extract the words and tags \n",
    "            - returns two list of indices, one for each\n",
    "            -> (wordids, tagids)\n",
    "            \"\"\"\n",
    "        wordids = list()\n",
    "        tagids = list()\n",
    "        for couple in sent:\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "            if wrd in self.X_index:\n",
    "                wordids.append(self.X_index[wrd])\n",
    "            else:\n",
    "                wordids.append(UNKid)\n",
    "            tagids.append(self.Y_index[tag])\n",
    "        return wordids, tagids\n",
    "\n",
    "    def observation_estimation(self, pair_counts):\n",
    "        \"\"\" Build the observation distribution: \n",
    "                observation_proba is the observation probablility matrix\n",
    "                    [b_ki],  b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "                \n",
    "                pair_counts is dictionary with \n",
    "                - key : a tuple (word,tag)\n",
    "                - value: the associated count\n",
    "                \n",
    "                We just need to fill the matrix and normalize the count in the right way: \n",
    "                - one column (i constant) is the distrib. of word given a tag\n",
    "                - just normalize the column, i.e sum over the row (axis=0)\n",
    "            \"\"\"\n",
    "        # fill with counts\n",
    "        for pair in pair_counts:\n",
    "            wrd = pair[0]  # get word\n",
    "            tag = pair[1]  # get tag\n",
    "            cpt = pair_counts[pair]  # get the count\n",
    "            # get word index (row), deal with OOV\n",
    "            k = 0  # for <unk>\n",
    "            if wrd in self.X_index:\n",
    "                k = self.X_index[wrd]\n",
    "            # get tag  index (column)\n",
    "            i = self.Y_index[tag]\n",
    "            # fill the matrix\n",
    "            self.observation_proba[k, i] = cpt\n",
    "        # normalize\n",
    "        self.observation_proba = self.observation_proba + self.smoothing_obs\n",
    "        self.observation_proba = self.observation_proba / self.observation_proba.sum(\n",
    "            axis=0).reshape(1, self.N)\n",
    "\n",
    "    def head_transition_estimation(self, head_trans_counts):\n",
    "        \"\"\" Buid the transition distribution \"\"\"\n",
    "        # fill with counts\n",
    "        for pair in head_trans_counts:\n",
    "            i = self.Y_index[pair[0]]\n",
    "            j = self.Y_index[pair[1]]\n",
    "            self.head_transition_proba[j, i] = head_trans_counts[pair]\n",
    "        # normalize\n",
    "        self.head_transition_proba = self.head_transition_proba + self.smoothing_obs\n",
    "        self.head_transition_proba = self.head_transition_proba / self.head_transition_proba.sum(\n",
    "            axis=0).reshape(1, self.N)\n",
    "\n",
    "    def transition_estimation(self, trans_counts):\n",
    "        \"\"\" Build the transition distribution \"\"\"\n",
    "        # fill with counts\n",
    "        for pair in trans_counts:\n",
    "            i = self.Y_index[pair[0][0]]\n",
    "            j = self.Y_index[pair[0][1]]\n",
    "            k = self.Y_index[pair[1]]\n",
    "            self.transition_proba[k, (i * self.N + j)] = trans_counts[pair]\n",
    "        # normalize\n",
    "        self.transition_proba = self.transition_proba + self.smoothing_obs\n",
    "        self.transition_proba = self.transition_proba / self.transition_proba.sum(\n",
    "            axis=0).reshape(1, self.N**2)\n",
    "\n",
    "    def init_estimation(self, init_counts):\n",
    "        \"\"\"Build the init. distribution\"\"\"\n",
    "        # fill with counts\n",
    "        for tag in init_counts:\n",
    "            i = self.Y_index[tag]\n",
    "            self.initial_state_proba[i] = init_counts[tag]\n",
    "        # normalize\n",
    "        self.initial_state_proba = self.initial_state_proba / sum(\n",
    "            self.initial_state_proba)\n",
    "\n",
    "    def supervised_training(self, pair_counts, head_trans_counts, trans_counts,\n",
    "                            init_counts):\n",
    "        \"\"\" Train the HMM's parameters. This function wraps everything \"\"\"\n",
    "        self.observation_estimation(pair_counts)\n",
    "        self.head_transition_estimation(head_trans_counts)\n",
    "        self.transition_estimation(trans_counts)\n",
    "        self.init_estimation(init_counts)\n",
    "\n",
    "    def viterbi(self, obsids):\n",
    "        \"\"\" Viterbi Algorithm : \n",
    "            Finding the most likely sequence of hidden states. \n",
    "            \"\"\"\n",
    "\n",
    "        T = len(obsids)\n",
    "\n",
    "        # Initialisation\n",
    "        delta = zeros(self.N, float)\n",
    "        tmp = zeros(self.N, float)\n",
    "        psi = zeros((T, self.N), int)\n",
    "\n",
    "        # Delta update\n",
    "        delta_t = zeros(self.N, float)\n",
    "\n",
    "        # Apply initial_state probabilities to the first frame\n",
    "        delta = self.observation_proba[obsids[0]] * self.initial_state_proba\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            if t == 1:\n",
    "                for i in range(self.N):\n",
    "                    for j in range(self.N):\n",
    "                        # Head\n",
    "                        tmp[j] = delta[j] * self.head_transition_proba[i, j]\n",
    "                    psi[t, i] = tmp.argmax()\n",
    "                    delta_t[i] = tmp.max() * self.observation_proba[obsids[t],\n",
    "                                                                    i]\n",
    "            else:\n",
    "                for i in range(self.N):\n",
    "                    for j in range(self.N):\n",
    "                        # Second\n",
    "                        tmp[j] = delta[j] * self.transition_proba[\n",
    "                            i, psi[t - 1, j] * self.N + j]\n",
    "                    psi[t, i] = tmp.argmax()\n",
    "                    delta_t[i] = tmp.max() * self.observation_proba[obsids[t],\n",
    "                                                                    i]\n",
    "\n",
    "            delta, delta_t = delta_t, delta\n",
    "\n",
    "        # Reconstruction\n",
    "        i_star = [delta.argmax()]\n",
    "        for psi_t in psi[-1:0:-1]:\n",
    "            i_star.append(psi_t[i_star[-1]])\n",
    "        i_star.reverse()\n",
    "\n",
    "        return i_star\n",
    "\n",
    "        \n",
    "    def print_error_rate(self, test):\n",
    "        \"\"\" Compute and print error rate on test\n",
    "            \"\"\"\n",
    "        nb_correct_before_hmm = 0\n",
    "        nb_correct_after_hmm = 0\n",
    "        total = 0\n",
    "        \n",
    "        for word in test:\n",
    "            obsids,statids = hmm.data2indices(word)\n",
    "            best_sequence = hmm.viterbi(obsids)\n",
    "\n",
    "            for (i,j) in zip(best_sequence,statids):\n",
    "                if i==j:\n",
    "                    nb_correct_after_hmm += 1\n",
    "                    \n",
    "            for(i,j) in zip(statids,obsids):\n",
    "                if i==j:\n",
    "                    nb_correct_before_hmm += 1\n",
    "\n",
    "            total += len(statids)\n",
    "\n",
    "\n",
    "        error_before_hmm = 100 - nb_correct_before_hmm * 100.0 / total\n",
    "        error_after_hmm = 100 - nb_correct_after_hmm * 100.0 / total\n",
    "\n",
    "        print(\"Error rate before HMM : {}%\".format(error_before_hmm))\n",
    "        print(\"Error rate after HMM : {}%\".format(error_after_hmm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compter les mots et les tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_counts(corpus):\n",
    "    \"\"\" \n",
    "    Build different count tables to train a HMM. Each count table is a dictionnary. \n",
    "    Returns: \n",
    "    * c_words: word counts\n",
    "    * c_tags: tag counts\n",
    "    * c_pairs: count of pairs (word,tag)\n",
    "    * c_transitions: count of tag bigram \n",
    "    * c_inits: count of tag found in the first position\n",
    "    \"\"\"\n",
    "    c_words = dict()\n",
    "    c_tags = dict()\n",
    "    c_pairs= dict()\n",
    "    c_transitions1 = dict()\n",
    "    c_transitions2 = dict()\n",
    "    c_inits = dict()\n",
    "    \n",
    "    for sent in corpus:\n",
    "        # we use i because of the transition counts\n",
    "        for i in range(len(sent)):\n",
    "            couple = sent[i]\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "            # word counts\n",
    "            if wrd in c_words:\n",
    "                c_words[wrd] = c_words[wrd] + 1\n",
    "            else:\n",
    "                c_words[wrd] = 1\n",
    "                \n",
    "            # tag counts\n",
    "            if tag in c_tags:\n",
    "                c_tags[tag] = c_tags[tag] + 1\n",
    "            else:\n",
    "                c_tags[tag] = 1\n",
    "                \n",
    "            # observation counts\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple] = c_pairs[couple] + 1\n",
    "            else:\n",
    "                c_pairs[couple] = 1\n",
    "            \n",
    "            # i >  1 -> transition counts\n",
    "            if i > 1:\n",
    "                trans = ((sent[i-2][1], sent[i-1][1]), tag) #(tag at t-2, tag at t-1, tag at t)\n",
    "                if trans in c_transitions2:\n",
    "                    c_transitions2[trans] = c_transitions2[trans] + 1\n",
    "                else:\n",
    "                    c_transitions2[trans] = 1\n",
    "                    \n",
    "            elif i == 1:\n",
    "                trans = (sent[i-1][1], tag)\n",
    "                if trans in c_transitions1:\n",
    "                    c_transitions1[trans] = c_transitions1[trans] + 1\n",
    "                else:\n",
    "                    c_transitions1[trans] = 1\n",
    "                    \n",
    "            # i == 0 -> counts for initial states\n",
    "            else:\n",
    "                if tag in c_inits:\n",
    "                    c_inits[tag] = c_inits[tag] + 1\n",
    "                else:\n",
    "                    c_inits[tag] = 1\n",
    "                    \n",
    "    return c_words, c_tags, c_pairs, c_transitions1, c_transitions2, c_inits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du vocabulaire (filtrage selon le nombre d'occurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(c, threshold):\n",
    "    \"\"\" \n",
    "    return a vocabulary by thresholding word counts. \n",
    "    inputs: \n",
    "    * c_words : a dictionnary that maps word to its counts\n",
    "    * threshold: count must be >= to the threshold to be included\n",
    "    \n",
    "    returns: \n",
    "    * a word list\n",
    "    \"\"\"\n",
    "    voc = list()\n",
    "    voc.append(UNK)\n",
    "    #for w in c:\n",
    "    #    if c[w] >= threshold:\n",
    "    #        voc.append(w)\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    for l1 in letters:\n",
    "        voc.append(l1)\n",
    "        for l2 in letters:\n",
    "            voc.append(l1+l2)\n",
    "    return voc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = ''\n",
    "\n",
    "# Données avec 10% de typos\n",
    "train10 = pickle.load(open(path+'train10.pkl', 'rb'))\n",
    "test10 = pickle.load(open(path+'test10.pkl', 'rb'))\n",
    "\n",
    "# Données avec 20% de typos\n",
    "train20 = pickle.load(open(path+'train20.pkl', 'rb'))\n",
    "test20 = pickle.load(open(path+'test20.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 't'), ('h', 'h'), ('e', 'e'), ('i', 'i'), ('r', 'r')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train10[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise des lettres en couple\n",
    "\n",
    "### [(b, b), (y, y)] devient [(by, by)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "\n",
    "def couple_data(data):\n",
    "    \"\"\" from [(b, b), (y, y)] to [(by, by)] \"\"\"\n",
    "    cdata = []\n",
    "    for w in data:\n",
    "        cw = []\n",
    "        for i in range(0, len(w), 2):\n",
    "            try:\n",
    "                cw.append((w[i][0] + w[i+1][0], w[i][1] + w[i+1][1]))\n",
    "            except:\n",
    "                cw.append((w[i][0], w[i][1]))\n",
    "        cdata.append(cw)\n",
    "    return cdata\n",
    "\n",
    "# Method 2\n",
    "# If suppresion : from [(b, b), (y, y)] to [(b, by)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train10 = couple_data(train10)\n",
    "test10 = couple_data(test10)\n",
    "train20 = couple_data(train20)\n",
    "test20 = couple_data(test20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('th', 'th'), ('ei', 'ei'), ('r', 'r')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train10[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les suppressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "\n",
    "def delete_char(train, percentage, with_substitution=False):\n",
    "    newtext = []\n",
    "    for word in train:\n",
    "        newword=[]\n",
    "        for i in range(len(word)):\n",
    "            # Suppression\n",
    "            if (len(word[i][0]) == 2) and (random()<percentage/100):\n",
    "                r = randrange(2) # 0 or 1\n",
    "                newword.append((word[i][1][r], word[i][1]))\n",
    "            else:\n",
    "                if(with_substitution):\n",
    "                    newword.append(word[i])\n",
    "                else:\n",
    "                    newword.append((word[i][1],word[i][1]))\n",
    "        newtext.append(newword)\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seulement des suppressions\n",
    "train10 = delete_char(train10, 10)\n",
    "test10 = delete_char(test10, 10)\n",
    "train20 = delete_char(train20, 20)\n",
    "test20 = delete_char(test20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('th', 'th'), ('em', 'em')]\n"
     ]
    }
   ],
   "source": [
    "print(train10[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases totale = 30558\n",
      "Nombre de phrases de train = 29057\n",
      "Nombre de phrases de test  = 1501\n"
     ]
    }
   ],
   "source": [
    "train = train10\n",
    "test = test10\n",
    "\n",
    "tot = len(train + test)\n",
    "print (\"Nombre de phrases totale = \" + str(tot))\n",
    "print (\"Nombre de phrases de train = \" + str(len(train)))\n",
    "print (\"Nombre de phrases de test  = \" + str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lettres  : 384\n",
      "Nombre de tags  : 381\n",
      "Nombre de paires: 904\n",
      "Nombre de init. : 207\n",
      "{'by': 195, 'th': 3657, 'ei': 187, 'r': 1113, 'ow': 137, 'n': 742, 'ac': 240, 'co': 661, 'un': 210, 't': 1312, 'vi': 325, 'ol': 117, 'en': 868, 'ce': 327, 'is': 1017, 'fo': 566, 'em': 449, 'a': 672, 'rm': 142, 'of': 1106, 'li': 361, 'be': 796, 'ra': 331, 'ti': 985, 'on': 1100, 'in': 1818, 'ot': 198, 'he': 371, 'wo': 288, 'rd': 80, 's': 1706, 'mm': 59, 'it': 734, 'ng': 404, 'ey': 218, 'br': 79, 'ea': 298, 'k': 52, 'ro': 290, 'ug': 88, 'h': 235, 'e': 3316, 'ps': 95, 'yc': 67, 'ho': 153, 'lo': 464, 'gi': 237, 'ca': 617, 'l': 561, 're': 1083, 'st': 948, 'ts': 224, 'at': 1134, 'ha': 524, 've': 659, 'tr': 215, 'ai': 136, 'ne': 545, 'd': 1394, 'to': 1191, 'us': 366, 'ar': 527, 'ov': 101, 'er': 711, 'so': 645, 'ci': 521, 'al': 925, 'iz': 36, 'ed': 534, 'es': 803, 'mo': 474, 'nf': 38, 'g': 413, 'an': 1340, 'rs': 169, 'nc': 194, 'fr': 223, 'ee': 144, 'bu': 255, 'ua': 39, 'll': 464, 'y': 1045, 'ju': 42, 'if': 167, 'io': 259, 'te': 833, 'ma': 679, 'am': 123, 'va': 112, 'lu': 128, 'ga': 108, 'ge': 230, 'cl': 96, 'm': 216, 'fi': 160, 'gh': 127, 'ag': 142, 'ns': 241, 'sm': 82, 'or': 552, 'ke': 183, 'we': 494, 'ny': 76, 'ob': 140, 'je': 13, 'ct': 207, 'ul': 257, 'go': 186, 'um': 24, 'b': 16, 'na': 162, 'il': 184, 'sk': 23, 'et': 338, 'ch': 585, 'le': 799, 'ft': 173, 'gy': 156, 'si': 313, 'tu': 147, 'mp': 122, 'x': 26, 'yt': 18, 'hi': 224, 'de': 600, 'sc': 134, 'ri': 385, 'pt': 24, 'ta': 244, 'se': 544, 'vo': 136, 'me': 515, 'ev': 209, 'ss': 284, 'da': 84, 'av': 53, 'la': 224, 'bl': 88, 'ly': 400, 'di': 375, 'ry': 80, 'hl': 4, 'tw': 39, 'o': 142, 'im': 197, 'po': 440, 'rt': 278, 'nd': 270, 'rn': 146, 'pr': 560, 'ms': 78, 'ou': 457, 'as': 333, 'wh': 384, 'w': 124, 'lf': 72, 'fe': 184, 'no': 608, 'pe': 480, 'ab': 195, 'wi': 471, 'sp': 98, 'ad': 174, 'ie': 288, 'gr': 196, 'ex': 210, 'nt': 487, 'ld': 78, 'ki': 81, 'ds': 19, 'oc': 86, 'hu': 171, 'gs': 72, 'ba': 47, 'bi': 59, 'og': 104, 'os': 134, 'el': 166, 'ic': 396, 'gn': 38, 'qu': 107, 'sa': 159, 'ur': 103, 'cu': 81, 'ef': 103, 'yo': 83, 'ir': 67, 'su': 368, 'cc': 32, 'ff': 83, 'au': 51, 'my': 28, 'pa': 186, 'ap': 124, 'hs': 20, 'id': 124, 'hy': 6, 'wa': 189, 'sh': 111, 'p': 34, 'fu': 82, 'ut': 59, 'bo': 58, 'ze': 29, 'cr': 147, 'ue': 83, 'ru': 60, 'gg': 26, 'lv': 75, 'do': 306, 'ni': 188, 'mu': 78, 'ph': 66, 'ys': 69, 'od': 93, 'za': 51, 'tt': 68, 'nm': 45, 'lt': 55, 'ib': 57, 'rv': 65, 'iv': 174, 'fa': 134, 'oi': 39, 'rr': 88, 'pl': 175, 'nk': 4, 'f': 24, 'gu': 65, 'op': 297, 'sf': 75, 'up': 57, 'pu': 148, 'rw': 13, 'pi': 52, 'eu': 6, 'nl': 13, 'dn': 11, 'ty': 151, 'rk': 81, 'lm': 15, 'om': 159, 'ui': 25, 'yi': 18, 'sl': 23, 'sw': 5, 'ud': 33, 'rl': 77, 'kn': 25, 'cy': 13, 'ip': 13, 'mb': 27, 'du': 209, 'mi': 163, 'jo': 38, 'ig': 35, 'ye': 35, 'rc': 45, 'nn': 45, 'rp': 37, 'dd': 12, 'dr': 80, 'nw': 3, 'rg': 61, 'ep': 56, 'xt': 10, 'ia': 76, 'tl': 44, 'ak': 19, 'wn': 35, 'ec': 134, 'fy': 13, 'rf': 29, 'ls': 23, 'eq': 36, 'sr': 23, 'xi': 10, 'xu': 9, 'c': 75, 'sy': 231, 'az': 2, 'tn': 6, 'wr': 11, 'ck': 44, 'bj': 9, 'pp': 56, 'ew': 10, 'dl': 6, 'u': 34, 'kd': 18, 'eg': 7, 'ya': 8, 'ok': 19, 'nu': 28, 'hb': 8, 'oo': 15, 'bs': 2, 'dp': 1, 'yb': 11, 'sn': 11, 'af': 35, 'fl': 19, 'oy': 16, 'ub': 10, 'ym': 3, 'lp': 12, 'xe': 1, 'rb': 5, 'ek': 10, 'dy': 11, 'lk': 12, 'ay': 21, 'zi': 4, 'eo': 35, 'pm': 22, 'fc': 5, 'py': 3, 'yl': 3, 'mn': 3, 'uc': 47, 'ae': 1, 'ws': 5, 'gl': 4, 'sb': 2, 'eh': 3, 'bm': 1, 'tc': 10, 'ku': 1, 'i': 13, 'nv': 9, 'ah': 4, 'yw': 3, 'xa': 2, 'ao': 3, 'td': 1, 'ks': 11, 'aw': 14, 'nh': 2, 'tm': 5, 'cs': 7, 'oh': 2, 'hn': 12, 'oa': 11, 'dv': 3, 'ht': 8, 'dc': 2, 'tf': 3, 'bt': 1, 'sg': 1, 'ja': 5, 'q': 1, 'gm': 4, 'ox': 5, 'kl': 3, 'nq': 3, 'dg': 6, 'lr': 2, 'vs': 4, 'lc': 2, 'tv': 1, 'vu': 1, 'ln': 2, 'oz': 1, 'aq': 2, 'ko': 3, 'nj': 1, 'bb': 3, 'oe': 2, 'bc': 1, 'rh': 5, 'tb': 1, 'dw': 4, 'eb': 1, 'dh': 1, 'ux': 1, 'uf': 1, 'yp': 3, 'mf': 1, 'ik': 1, 'yz': 1, 'iq': 1}\n",
      "Vocabulaire :703\n"
     ]
    }
   ],
   "source": [
    "cwords,ctags,cpairs,ctrans1,ctrans2,cinits = make_counts(train)\n",
    "print (\"Nombre de lettres  : \"+str(len(cwords)))\n",
    "print (\"Nombre de tags  : \"+str(len(ctags)))\n",
    "print (\"Nombre de paires: \"+str(len(cpairs)))\n",
    "print (\"Nombre de init. : \"+str(len(cinits)))\n",
    "print (ctags)\n",
    "vocab = make_vocab(cwords,10)\n",
    "print (\"Vocabulaire :\"+str(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du HMM et apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "703 states\n",
      "703 observations\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=vocab, \n",
    "          observation_list=vocab,\n",
    "          smoothing_obs = 0.001)\n",
    "\n",
    "hmm.supervised_training(cpairs,ctrans1,ctrans2,cinits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tests sur test10 avec seulement des suppressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate before HMM : 8.02901450725362%\n",
      "Error rate after HMM : 2.776388194097052%\n"
     ]
    }
   ],
   "source": [
    "hmm.print_error_rate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tests sur test20 avec seulement des suppressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "703 states\n",
      "703 observations\n",
      "Error rate before HMM : 16.752718883884427%\n",
      "Error rate after HMM : 5.108206085905749%\n"
     ]
    }
   ],
   "source": [
    "cwords,ctags,cpairs,ctrans1,ctrans2,cinits = make_counts(train20)\n",
    "\n",
    "hmm = HMM(state_list=vocab, \n",
    "          observation_list=vocab, \n",
    "          smoothing_obs = 0.001)\n",
    "\n",
    "hmm.supervised_training(cpairs,ctrans1,ctrans2,cinits)\n",
    "hmm.print_error_rate(test20)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
